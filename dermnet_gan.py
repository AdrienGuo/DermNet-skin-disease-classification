# -*- coding: utf-8 -*-
"""Dermnet_DS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WbzRlCkXGChdkqaVTjPRtADPpATJSHze
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf
import pandas as pd
import seaborn as sns

from tensorflow import keras
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, MaxPooling2D, Activation, ZeroPadding2D
from tensorflow.keras import layers
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.convolutional import UpSampling2D, Conv2D
from sklearn.metrics import classification_report, confusion_matrix
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras import regularizers
from keras import metrics

from keras.optimizers import Adam       # 找 optimizer 的問題，結果是版本的問題
from keras.layers import LeakyReLU

import glob 

from sklearn.model_selection import train_test_split
from keras.models import Sequential, Model

Rosacea = glob.glob('./Dermnet/Acne_and_Rosacea_Photos/*.*')
Nail = glob.glob('./Dermnet/Nail_Fungus_and_other_Nail_Disease/*.*')
Seborrheic = glob.glob('./Dermnet/Seborrheic_Keratoses_and_other_Benign_Tumors/*.*')
Urticaria = glob.glob('./Dermnet/Urticaria_Hives/*.*')
Vasculitis = glob.glob('./Dermnet/Vasculitis_Photos/*.*')



# List the number of images in each symptom.
train_path = "./data/train/"
test_path = "./data/test/"

train_symptom_dir = os.listdir(train_path)
test_symptom_dir = os.listdir(test_path)

NUM_CLASSES = 5
class_names = [train_symptom_dir[i] for i in range(NUM_CLASSES)]
print(class_names)

print("="*100)
print("Train dataset\n")
for symptom_name in class_names:
    print("{}: {}".format(symptom_name, len(os.listdir(train_path + symptom_name))))

print("="*100)
print("Test dataset\n")
for symptom_name in class_names:
    print("{}: {}".format(symptom_name, len(os.listdir(test_path + symptom_name))))

data = []
labels = []
image_size = 40
for i in Rosacea:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (image_size,image_size))
    image=np.array(image)
    data.append(image)
    labels.append(0)
for i in Nail:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (image_size,image_size))
    image=np.array(image)
    data.append(image)
    labels.append(1)  
for i in Seborrheic:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (image_size,image_size))
    image=np.array(image)
    data.append(image)
    labels.append(2)
for i in Urticaria:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (image_size,image_size))
    image=np.array(image)
    data.append(image)
    labels.append(3)
for i in Vasculitis:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (image_size,image_size))
    image=np.array(image)
    data.append(image)
    labels.append(4)    

data = np.array(data)
labels = np.array(labels)

def discriminator():
   start = Input(shape = (image_size,image_size,3))

   d1 = Conv2D(8, kernel_size=3, strides=2, input_shape=(image_size,image_size,3), padding="same")(start)
   d1 = LeakyReLU(alpha=0.2)(d1)

   d2 = Conv2D(16, kernel_size=3, strides=2, padding="same")(d1)
   d2 = ZeroPadding2D(padding=((0, 1), (0, 1)))(d2)
   d2 = LeakyReLU(alpha=0.2)(d2)

   d3 = Conv2D(32, kernel_size=3, strides=2, padding="same")(d2)
   d3 = LeakyReLU(alpha=0.2)(d3)

   d4 = Flatten()(d3)
   output = Dense(1, activation='sigmoid')(d4)

   return Model(start, output)

def generator(input_dim=32):
   noise = Input(shape=(input_dim,))
   number = int(image_size/4)
   g1 = Dense(number*number*32, activation="relu", input_dim=input_dim)(noise)
   g1 = Reshape((number, number, 32))(g1)

   g2 = UpSampling2D()(g1)
   g2 = Conv2D(32, kernel_size=3, padding="same", activation="relu")(g2)

   g3 = UpSampling2D()(g2)
   g3 = Conv2D(16, kernel_size=3, padding="same", activation="relu")(g3)

   g4 = Conv2D(3, kernel_size=3, padding="same")(g3)
   img = Activation("tanh")(g4)

   print(img.shape)
   print(type(Model(noise, img)))

   return Model(noise, img)

def obtain_dataset(data,labels,selected_class):
    #Load Data  
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)
    
    x_train = X_train[y_train==selected_class][:,:,:]
    y_train = y_train[y_train==selected_class][:]
    x_test = X_test[y_test==selected_class][:,:,:]
    y_test = y_test[y_test==selected_class][:]

    # Set data shape, type and groundtruth
    X_train = X_train.astype('float32').reshape(-1, image_size, image_size, 1)
    X_train /= 255

    X_test = X_test.astype('float32').reshape(-1, image_size, image_size, 1)
    X_test /= 255

    y_train = y_train.astype('uint8').reshape((-1,1))
    y_test = y_test.astype('uint8').reshape((-1,1))

    return x_train, y_train, x_test, y_test

def build_and_train_GAN(epochs, batch_size):
  
  # define hyper-parameter for GAN
  optimizer_GEN = Adam(0.0001, 0.5)
  optimizer_DIS = Adam(0.0004, 0.5)

  # build a discriminator named 'Dis' 
  Dis = discriminator()

  # compile 'Dis'
  Dis.compile(loss='binary_crossentropy', optimizer=optimizer_DIS, metrics=['accuracy'])

  # build a generator named 'Gen' with input random_vector
  Gen = generator(random_vector)        # Gen: Model(noise, img)
  print("Gen type: ", type(Gen))

  # Generator training route
  start = Input(shape=(random_vector,))
  fake_image = Gen(start)               # fake_image shape: (None, 28, 28, 1)
  Dis.trainable = False
  decide = Dis(fake_image)              # decide shape: (None, 1)
  comb_model = Model(start, decide)     # I guess it is to combine the two models.
  comb_model.compile(loss='binary_crossentropy', optimizer=optimizer_GEN)

  valid = np.ones((batch_size, 1))
  fake = np.zeros((batch_size, 1))


  # Use train_on_batch instead of model.fit
  for epoch in range(epochs):
      count = 0
      for time in range(x_train_Urticaria.shape[0]//batch_size):
          # Get Fake & Real samples for discriminator from generator & dataset
          noise = np.random.normal(0, 1, (batch_size, random_vector))
          gen_images = Gen.predict(noise)
          train_images = x_train_Urticaria[count:count+batch_size, :, :, :]

          # Training Discriminator
          d_loss_real = Dis.train_on_batch(train_images, valid)
          d_loss_fake = Dis.train_on_batch(gen_images, fake)
          d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

          # Training generator
          noise = np.random.normal(0, 1, (batch_size, random_vector))
          g_loss = comb_model.train_on_batch(noise, valid)

          print("%d-%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, time, d_loss[0], 100*d_loss[1], g_loss))

          count += batch_size

  return Gen

def create_fake_images(number_of_fake_images):
  fake_images = np.empty((0,image_size,image_size,3))       # 4 dimension ?
  for ni in range(number_of_fake_images):
    noise = np.random.normal(0, 1, (1,random_vector))       # 0: mean, 1: std, (1, random_vector): [1, random_vector] array
    temp = Gen.predict(noise)
    temp = 0.5 * temp + 0.5 
    fake_images = np.append(fake_images,temp)
  
  return fake_images

# [1] Define the hyper-parameters
random_vector = 32

x_train_Rosacea, y_train_Rosacea, x_test_Rosacea, y_test_Rosacea = obtain_dataset(data, labels,selected_class=0)
x_train_Nail, y_train_Nail, x_test_Nail, y_test_Nail = obtain_dataset(data, labels,selected_class=1)
x_train_Seborrheic, y_train_Seborrheic, x_test_Seborrheic, y_test_Seborrheic = obtain_dataset(data, labels,selected_class=2)
x_train_Urticaria, y_train_Urticaria, x_test_Urticaria, y_test_Urticaria = obtain_dataset(data, labels,selected_class=3)
x_train_Vasculitis, y_train_Vasculitis, x_test_Vasculitis, y_test_Vasculitis = obtain_dataset(data, labels,selected_class=4)

print("class Rosacea: ", x_train_Rosacea.shape, x_test_Rosacea.shape, y_train_Rosacea.shape, y_test_Rosacea.shape)
print("class Nail: ", x_train_Nail.shape, x_test_Nail.shape, y_train_Nail.shape, y_test_Nail.shape)
print("class Seborrheic: ", x_train_Seborrheic.shape, x_test_Seborrheic.shape, y_train_Seborrheic.shape, y_test_Seborrheic.shape)
print("class Urticaria: ", x_train_Urticaria.shape, x_test_Urticaria.shape, y_train_Urticaria.shape, y_test_Urticaria.shape)
print("class Vasculitis: ", x_train_Vasculitis.shape, x_test_Vasculitis.shape, y_train_Vasculitis.shape, y_test_Vasculitis.shape)

def classifier(): ##Change Model
    start = Input(shape = (image_size,image_size,3))
    
    c1 = Conv2D(8, kernel_size=3, strides=2, input_shape=(image_size,image_size,3), padding='same', activation='relu')(start)
    c1 = MaxPooling2D(pool_size=(2, 2))(c1)

    c2 = Conv2D(16, kernel_size=3, strides=2, input_shape=(image_size,image_size,3), padding='same', activation='relu')(c1)
    c2 = MaxPooling2D(pool_size=(2, 2))(c2)

    flat = Flatten()(c2)
    hidden1 = Dense(64, activation='relu')(flat)
    output = Dense(5, activation='softmax')(hidden1)

    return Model(inputs=start, outputs=output)


def build_and_train_classifier(epochs, batch_size):
  # build a classifier named 'model'
  model = classifier()

  # compile 'model'
  optimizer_model = Adam(learning_rate=0.0001, decay=0.5)     # 傻眼，出了一個 adam_v2
  model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer_model, metrics=['accuracy'])

  # train the classifier
  history = model.fit(x_train, y_train, epochs=epochs, validation_split=0.1)

  # test the classifier
  y_pred = model.predict(x_test)
  y_pred = y_pred.argmax(axis=-1)

  return y_pred

def prediction(save_path):
    y_pred = build_and_train_classifier(epochs=20, batch_size=32)

    con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()
    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)

    con_mat_df = pd.DataFrame(con_mat_norm, index=class_names, columns=class_names)
    figure = plt.figure(figsize=(10, 10))
    sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()
    plt.savefig(save_path)

# [3] Create imbalanced dataset by merging class '0' and '1' 
x_train=np.concatenate((x_train_Rosacea, x_train_Nail, x_train_Seborrheic, x_train_Urticaria, x_train_Vasculitis)) 
x_test=np.concatenate((x_test_Rosacea, x_test_Nail, x_test_Seborrheic, x_test_Urticaria, x_test_Vasculitis))
y_train=np.concatenate((y_train_Rosacea, y_train_Nail, y_train_Seborrheic, y_train_Urticaria, y_train_Vasculitis))
y_test=np.concatenate((y_test_Rosacea, y_test_Nail, y_test_Seborrheic, y_test_Urticaria, y_test_Vasculitis))
print("merged All data: ", x_train.shape, x_test.shape, y_train.shape, y_test.shape)

# without GAN
build_and_train_classifier(epochs=20, batch_size=32)
save_path = "./image/con_mat_wo_GAN.png"
prediction(save_path)


Gen = build_and_train_GAN(epochs=5, batch_size=32)

# [6] Create fake data for class '0' 
number_of_fake_images = 800

# TODO: call function create_fake_images with given number of fake images, save the output to a variable named 'fake_images'
fake_images = create_fake_images(number_of_fake_images)

fake_images = fake_images.reshape((-1, image_size, image_size, 3))      # -1: unspecified
for i in range(100):
  # define subplot
  plt.subplot(10, 10, 1 + i)
  # turn off axis
  plt.axis('off')
  # plot raw pixel data
  plt.imshow(fake_images[i])
plt.show()


# [7] Merge fake data with the original data
test_size = 0.2
train_num = int(number_of_fake_images*(1-test_size))
#test_num = int(number_of_fake_images*test_size)
x_train = np.concatenate((x_train, fake_images[:train_num,:,:]))
# x_test = np.concatenate((x_test, fake_images[train_num:,:,:]))                # commented out
y_train = np.concatenate((y_train, np.zeros(train_num).reshape((-1,1))))
# y_test = np.concatenate((y_test, np.zeros(test_num).reshape((-1,1))))         # commented out
print("final data: ", x_train.shape, x_test.shape, y_train.shape, y_test.shape)

print("final data: ", x_train.shape, x_test.shape, y_train.shape, y_test.shape)

y_pred = build_and_train_classifier(epochs=20, batch_size=32)

con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()
con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)

con_mat_df = pd.DataFrame(con_mat_norm, index=class_names, columns=class_names)
figure = plt.figure(figsize=(10, 10))
sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
plt.savefig("./image/con_mat.png")
